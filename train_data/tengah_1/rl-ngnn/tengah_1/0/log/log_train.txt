[2024-10-27 14:32:51,523] data_dir:data/tengah_1
[2024-10-27 14:32:51,523] id: tengah_1
[2024-10-27 14:32:51,523] seed: 0
[2024-10-27 14:32:51,523] objectives_plan: 
[2024-10-27 14:32:51,523] init_plan: 
[2024-10-27 14:32:51,523] env_specs: {}
[2024-10-27 14:32:51,523] reward_specs: {'dis_weight': 0.5, 'cost_weight': 0.5, 'build_ration': 0.1}
[2024-10-27 14:32:51,523] obs_specs: {}
[2024-10-27 14:32:51,523] agent_specs: {'batch_stage': False}
[2024-10-27 14:32:51,523] gamma: 0.9
[2024-10-27 14:32:51,523] tau: 0.0
[2024-10-27 14:32:51,523] state_encoder_specs: {'state_encoder_hidden_size': [16, 4], 'gcn_node_dim': 16, 'num_gcn_layers': 2, 'num_edge_fc_layers': 1, 'max_num_nodes': 1000, 'max_num_edges': 3000, 'num_attention_heads': 1}
[2024-10-27 14:32:51,523] policy_specs: {'policy_land_use_head_hidden_size': [32, 1], 'policy_road_head_hidden_size': [32, 1]}
[2024-10-27 14:32:51,523] value_specs: {'value_head_hidden_size': [32, 32, 1]}
[2024-10-27 14:32:51,523] lr: 0.0004
[2024-10-27 14:32:51,523] weightdecay: 0.0
[2024-10-27 14:32:51,523] eps: 1e-05
[2024-10-27 14:32:51,523] value_pred_coef: 0.5
[2024-10-27 14:32:51,523] entropy_coef: 0.01
[2024-10-27 14:32:51,523] clip_epsilon: 0.2
[2024-10-27 14:32:51,523] max_num_iterations: 1
[2024-10-27 14:32:51,523] num_episodes_per_iteration: 1200
[2024-10-27 14:32:51,523] max_sequence_length: 33
[2024-10-27 14:32:51,523] num_optim_epoch: 4
[2024-10-27 14:32:51,523] mini_batch_size: 1024
[2024-10-27 14:32:51,523] save_model_interval: 1
[2024-10-27 14:32:56,438] 0	T_sample 1.46	T_update 0.35	T_eval 2.63	ETA 0:00:00	train_R_eps -2.81	eval_R_eps -2.65	tengah_1	
[2024-10-27 14:32:56,442] save best checkpoint with rewards -2.65!
[2024-10-27 14:35:26,427] data_dir:data/tengah_1
[2024-10-27 14:35:26,427] id: tengah_1
[2024-10-27 14:35:26,427] seed: 0
[2024-10-27 14:35:26,427] objectives_plan: 
[2024-10-27 14:35:26,427] init_plan: 
[2024-10-27 14:35:26,427] env_specs: {}
[2024-10-27 14:35:26,427] reward_specs: {'dis_weight': 0.5, 'cost_weight': 0.5, 'build_ration': 0.1}
[2024-10-27 14:35:26,427] obs_specs: {}
[2024-10-27 14:35:26,427] agent_specs: {'batch_stage': False}
[2024-10-27 14:35:26,427] gamma: 0.9
[2024-10-27 14:35:26,427] tau: 0.0
[2024-10-27 14:35:26,427] state_encoder_specs: {'state_encoder_hidden_size': [16, 4], 'gcn_node_dim': 16, 'num_gcn_layers': 2, 'num_edge_fc_layers': 1, 'max_num_nodes': 1000, 'max_num_edges': 3000, 'num_attention_heads': 1}
[2024-10-27 14:35:26,427] policy_specs: {'policy_land_use_head_hidden_size': [32, 1], 'policy_road_head_hidden_size': [32, 1]}
[2024-10-27 14:35:26,427] value_specs: {'value_head_hidden_size': [32, 32, 1]}
[2024-10-27 14:35:26,427] lr: 0.0004
[2024-10-27 14:35:26,427] weightdecay: 0.0
[2024-10-27 14:35:26,427] eps: 1e-05
[2024-10-27 14:35:26,427] value_pred_coef: 0.5
[2024-10-27 14:35:26,427] entropy_coef: 0.01
[2024-10-27 14:35:26,427] clip_epsilon: 0.2
[2024-10-27 14:35:26,427] max_num_iterations: 1
[2024-10-27 14:35:26,427] num_episodes_per_iteration: 1200
[2024-10-27 14:35:26,427] max_sequence_length: 33
[2024-10-27 14:35:26,428] num_optim_epoch: 4
[2024-10-27 14:35:26,428] mini_batch_size: 1024
[2024-10-27 14:35:26,428] save_model_interval: 1
[2024-10-27 14:35:31,350] 0	T_sample 1.48	T_update 0.36	T_eval 2.58	ETA 0:00:00	train_R_eps -2.81	eval_R_eps -2.65	tengah_1	
[2024-10-27 14:35:31,353] save best checkpoint with rewards -2.65!
[2024-10-27 14:36:36,517] data_dir:data/tengah_1
[2024-10-27 14:36:36,518] id: tengah_1
[2024-10-27 14:36:36,518] seed: 0
[2024-10-27 14:36:36,518] objectives_plan: 
[2024-10-27 14:36:36,518] init_plan: 
[2024-10-27 14:36:36,518] env_specs: {}
[2024-10-27 14:36:36,518] reward_specs: {'dis_weight': 0.5, 'cost_weight': 0.5, 'build_ration': 0.1}
[2024-10-27 14:36:36,518] obs_specs: {}
[2024-10-27 14:36:36,518] agent_specs: {'batch_stage': False}
[2024-10-27 14:36:36,518] gamma: 0.9
[2024-10-27 14:36:36,518] tau: 0.0
[2024-10-27 14:36:36,518] state_encoder_specs: {'state_encoder_hidden_size': [16, 4], 'gcn_node_dim': 16, 'num_gcn_layers': 2, 'num_edge_fc_layers': 1, 'max_num_nodes': 1000, 'max_num_edges': 3000, 'num_attention_heads': 1}
[2024-10-27 14:36:36,518] policy_specs: {'policy_land_use_head_hidden_size': [32, 1], 'policy_road_head_hidden_size': [32, 1]}
[2024-10-27 14:36:36,518] value_specs: {'value_head_hidden_size': [32, 32, 1]}
[2024-10-27 14:36:36,518] lr: 0.0004
[2024-10-27 14:36:36,518] weightdecay: 0.0
[2024-10-27 14:36:36,518] eps: 1e-05
[2024-10-27 14:36:36,518] value_pred_coef: 0.5
[2024-10-27 14:36:36,518] entropy_coef: 0.01
[2024-10-27 14:36:36,518] clip_epsilon: 0.2
[2024-10-27 14:36:36,518] max_num_iterations: 1
[2024-10-27 14:36:36,518] num_episodes_per_iteration: 1200
[2024-10-27 14:36:36,518] max_sequence_length: 33
[2024-10-27 14:36:36,518] num_optim_epoch: 4
[2024-10-27 14:36:36,518] mini_batch_size: 1024
[2024-10-27 14:36:36,518] save_model_interval: 1
[2024-10-27 14:36:41,385] 0	T_sample 1.46	T_update 0.34	T_eval 2.59	ETA 0:00:00	train_R_eps -2.81	eval_R_eps -2.65	tengah_1	
[2024-10-27 14:36:41,389] save best checkpoint with rewards -2.65!
[2024-10-27 14:46:36,889] data_dir:data/tengah_1
[2024-10-27 14:46:36,889] id: tengah_1
[2024-10-27 14:46:36,889] seed: 0
[2024-10-27 14:46:36,889] objectives_plan: 
[2024-10-27 14:46:36,889] init_plan: 
[2024-10-27 14:46:36,889] env_specs: {}
[2024-10-27 14:46:36,889] reward_specs: {'dis_weight': 0.5, 'cost_weight': 0.5, 'build_ration': 0.1}
[2024-10-27 14:46:36,889] obs_specs: {}
[2024-10-27 14:46:36,890] agent_specs: {'batch_stage': False}
[2024-10-27 14:46:36,890] gamma: 0.9
[2024-10-27 14:46:36,890] tau: 0.0
[2024-10-27 14:46:36,890] state_encoder_specs: {'state_encoder_hidden_size': [16, 4], 'gcn_node_dim': 16, 'num_gcn_layers': 2, 'num_edge_fc_layers': 1, 'max_num_nodes': 1000, 'max_num_edges': 3000, 'num_attention_heads': 1}
[2024-10-27 14:46:36,890] policy_specs: {'policy_land_use_head_hidden_size': [32, 1], 'policy_road_head_hidden_size': [32, 1]}
[2024-10-27 14:46:36,890] value_specs: {'value_head_hidden_size': [32, 32, 1]}
[2024-10-27 14:46:36,890] lr: 0.0004
[2024-10-27 14:46:36,890] weightdecay: 0.0
[2024-10-27 14:46:36,890] eps: 1e-05
[2024-10-27 14:46:36,890] value_pred_coef: 0.5
[2024-10-27 14:46:36,890] entropy_coef: 0.01
[2024-10-27 14:46:36,890] clip_epsilon: 0.2
[2024-10-27 14:46:36,890] max_num_iterations: 1
[2024-10-27 14:46:36,890] num_episodes_per_iteration: 1200
[2024-10-27 14:46:36,890] max_sequence_length: 33
[2024-10-27 14:46:36,890] num_optim_epoch: 4
[2024-10-27 14:46:36,890] mini_batch_size: 1024
[2024-10-27 14:46:36,890] save_model_interval: 1
[2024-10-27 14:46:41,832] 0	T_sample 1.49	T_update 0.34	T_eval 2.61	ETA 0:00:00	train_R_eps -2.81	eval_R_eps -2.65	tengah_1	
[2024-10-27 14:46:41,836] save best checkpoint with rewards -2.65!
[2024-10-27 14:47:14,299] data_dir:data/tengah_1
[2024-10-27 14:47:14,299] id: tengah_1
[2024-10-27 14:47:14,300] seed: 0
[2024-10-27 14:47:14,300] objectives_plan: 
[2024-10-27 14:47:14,300] init_plan: 
[2024-10-27 14:47:14,300] env_specs: {}
[2024-10-27 14:47:14,300] reward_specs: {'dis_weight': 0.5, 'cost_weight': 0.5, 'build_ration': 0.1}
[2024-10-27 14:47:14,300] obs_specs: {}
[2024-10-27 14:47:14,300] agent_specs: {'batch_stage': False}
[2024-10-27 14:47:14,300] gamma: 0.9
[2024-10-27 14:47:14,300] tau: 0.0
[2024-10-27 14:47:14,300] state_encoder_specs: {'state_encoder_hidden_size': [16, 4], 'gcn_node_dim': 16, 'num_gcn_layers': 2, 'num_edge_fc_layers': 1, 'max_num_nodes': 1000, 'max_num_edges': 3000, 'num_attention_heads': 1}
[2024-10-27 14:47:14,300] policy_specs: {'policy_land_use_head_hidden_size': [32, 1], 'policy_road_head_hidden_size': [32, 1]}
[2024-10-27 14:47:14,300] value_specs: {'value_head_hidden_size': [32, 32, 1]}
[2024-10-27 14:47:14,300] lr: 0.0004
[2024-10-27 14:47:14,300] weightdecay: 0.0
[2024-10-27 14:47:14,300] eps: 1e-05
[2024-10-27 14:47:14,300] value_pred_coef: 0.5
[2024-10-27 14:47:14,300] entropy_coef: 0.01
[2024-10-27 14:47:14,300] clip_epsilon: 0.2
[2024-10-27 14:47:14,300] max_num_iterations: 1
[2024-10-27 14:47:14,300] num_episodes_per_iteration: 1200
[2024-10-27 14:47:14,300] max_sequence_length: 33
[2024-10-27 14:47:14,300] num_optim_epoch: 4
[2024-10-27 14:47:14,300] mini_batch_size: 1024
[2024-10-27 14:47:14,300] save_model_interval: 1
[2024-10-27 14:47:19,228] 0	T_sample 1.49	T_update 0.34	T_eval 2.61	ETA 0:00:00	train_R_eps -2.81	eval_R_eps -2.65	tengah_1	
[2024-10-27 14:47:19,232] save best checkpoint with rewards -2.65!
[2024-10-27 14:47:26,850] data_dir:data/tengah_1
[2024-10-27 14:47:26,851] id: tengah_1
[2024-10-27 14:47:26,851] seed: 0
[2024-10-27 14:47:26,851] objectives_plan: 
[2024-10-27 14:47:26,851] init_plan: 
[2024-10-27 14:47:26,851] env_specs: {}
[2024-10-27 14:47:26,851] reward_specs: {'dis_weight': 0.5, 'cost_weight': 0.5, 'build_ration': 0.1}
[2024-10-27 14:47:26,851] obs_specs: {}
[2024-10-27 14:47:26,851] agent_specs: {'batch_stage': False}
[2024-10-27 14:47:26,851] gamma: 0.9
[2024-10-27 14:47:26,851] tau: 0.0
[2024-10-27 14:47:26,851] state_encoder_specs: {'state_encoder_hidden_size': [16, 4], 'gcn_node_dim': 16, 'num_gcn_layers': 2, 'num_edge_fc_layers': 1, 'max_num_nodes': 1000, 'max_num_edges': 3000, 'num_attention_heads': 1}
[2024-10-27 14:47:26,851] policy_specs: {'policy_land_use_head_hidden_size': [32, 1], 'policy_road_head_hidden_size': [32, 1]}
[2024-10-27 14:47:26,851] value_specs: {'value_head_hidden_size': [32, 32, 1]}
[2024-10-27 14:47:26,851] lr: 0.0004
[2024-10-27 14:47:26,851] weightdecay: 0.0
[2024-10-27 14:47:26,851] eps: 1e-05
[2024-10-27 14:47:26,851] value_pred_coef: 0.5
[2024-10-27 14:47:26,851] entropy_coef: 0.01
[2024-10-27 14:47:26,851] clip_epsilon: 0.2
[2024-10-27 14:47:26,851] max_num_iterations: 1
[2024-10-27 14:47:26,851] num_episodes_per_iteration: 1200
[2024-10-27 14:47:26,851] max_sequence_length: 33
[2024-10-27 14:47:26,851] num_optim_epoch: 4
[2024-10-27 14:47:26,851] mini_batch_size: 1024
[2024-10-27 14:47:26,851] save_model_interval: 1
[2024-10-27 14:47:31,747] 0	T_sample 1.48	T_update 0.34	T_eval 2.59	ETA 0:00:00	train_R_eps -2.81	eval_R_eps -2.65	tengah_1	
[2024-10-27 14:47:31,751] save best checkpoint with rewards -2.65!
[2024-10-27 15:00:25,287] data_dir:data/tengah_1
[2024-10-27 15:00:25,287] id: tengah_1
[2024-10-27 15:00:25,287] seed: 0
[2024-10-27 15:00:25,288] objectives_plan: 
[2024-10-27 15:00:25,288] init_plan: 
[2024-10-27 15:00:25,288] env_specs: {}
[2024-10-27 15:00:25,288] reward_specs: {'dis_weight': 0.5, 'cost_weight': 0.5, 'build_ration': 0.1}
[2024-10-27 15:00:25,288] obs_specs: {}
[2024-10-27 15:00:25,288] agent_specs: {'batch_stage': False}
[2024-10-27 15:00:25,288] gamma: 0.9
[2024-10-27 15:00:25,288] tau: 0.0
[2024-10-27 15:00:25,288] state_encoder_specs: {'state_encoder_hidden_size': [16, 4], 'gcn_node_dim': 16, 'num_gcn_layers': 2, 'num_edge_fc_layers': 1, 'max_num_nodes': 1000, 'max_num_edges': 3000, 'num_attention_heads': 1}
[2024-10-27 15:00:25,288] policy_specs: {'policy_land_use_head_hidden_size': [32, 1], 'policy_road_head_hidden_size': [32, 1]}
[2024-10-27 15:00:25,288] value_specs: {'value_head_hidden_size': [32, 32, 1]}
[2024-10-27 15:00:25,288] lr: 0.0004
[2024-10-27 15:00:25,288] weightdecay: 0.0
[2024-10-27 15:00:25,288] eps: 1e-05
[2024-10-27 15:00:25,288] value_pred_coef: 0.5
[2024-10-27 15:00:25,288] entropy_coef: 0.01
[2024-10-27 15:00:25,288] clip_epsilon: 0.2
[2024-10-27 15:00:25,288] max_num_iterations: 1
[2024-10-27 15:00:25,288] num_episodes_per_iteration: 1200
[2024-10-27 15:00:25,288] max_sequence_length: 33
[2024-10-27 15:00:25,288] num_optim_epoch: 4
[2024-10-27 15:00:25,288] mini_batch_size: 1024
[2024-10-27 15:00:25,288] save_model_interval: 1
[2024-10-27 15:00:30,203] 0	T_sample 1.49	T_update 0.32	T_eval 2.62	ETA 0:00:00	train_R_eps -2.81	eval_R_eps -2.65	tengah_1	
[2024-10-27 15:00:30,206] save best checkpoint with rewards -2.65!
[2024-10-27 15:02:18,888] data_dir:data/tengah_1
[2024-10-27 15:02:18,888] id: tengah_1
[2024-10-27 15:02:18,888] seed: 0
[2024-10-27 15:02:18,888] objectives_plan: 
[2024-10-27 15:02:18,888] init_plan: 
[2024-10-27 15:02:18,888] env_specs: {}
[2024-10-27 15:02:18,888] reward_specs: {'dis_weight': 0.5, 'cost_weight': 0.5, 'build_ration': 0.1}
[2024-10-27 15:02:18,888] obs_specs: {}
[2024-10-27 15:02:18,888] agent_specs: {'batch_stage': False}
[2024-10-27 15:02:18,888] gamma: 0.9
[2024-10-27 15:02:18,889] tau: 0.0
[2024-10-27 15:02:18,889] state_encoder_specs: {'state_encoder_hidden_size': [16, 4], 'gcn_node_dim': 16, 'num_gcn_layers': 2, 'num_edge_fc_layers': 1, 'max_num_nodes': 1000, 'max_num_edges': 3000, 'num_attention_heads': 1}
[2024-10-27 15:02:18,889] policy_specs: {'policy_land_use_head_hidden_size': [32, 1], 'policy_road_head_hidden_size': [32, 1]}
[2024-10-27 15:02:18,889] value_specs: {'value_head_hidden_size': [32, 32, 1]}
[2024-10-27 15:02:18,889] lr: 0.0004
[2024-10-27 15:02:18,889] weightdecay: 0.0
[2024-10-27 15:02:18,889] eps: 1e-05
[2024-10-27 15:02:18,889] value_pred_coef: 0.5
[2024-10-27 15:02:18,889] entropy_coef: 0.01
[2024-10-27 15:02:18,889] clip_epsilon: 0.2
[2024-10-27 15:02:18,889] max_num_iterations: 1
[2024-10-27 15:02:18,889] num_episodes_per_iteration: 1200
[2024-10-27 15:02:18,889] max_sequence_length: 33
[2024-10-27 15:02:18,889] num_optim_epoch: 4
[2024-10-27 15:02:18,889] mini_batch_size: 1024
[2024-10-27 15:02:18,889] save_model_interval: 1
[2024-10-27 15:02:23,859] 0	T_sample 1.50	T_update 0.35	T_eval 2.63	ETA 0:00:00	train_R_eps -2.81	eval_R_eps -2.65	tengah_1	
[2024-10-27 15:02:23,863] save best checkpoint with rewards -2.65!
